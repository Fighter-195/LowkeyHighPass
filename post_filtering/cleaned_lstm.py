# -*- coding: utf-8 -*-
"""cleaned_lstm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eSkBdmEK4D854-10gwdL88TTBLevQlrD
"""

# -*- coding: utf-8 -*-
"""training.ipynb - Cleaned Version

Automatically generated by Colab.
Original file is located at
    https://colab.research.google.com/drive/1rHflctP54ietfaQ13b240x6Zgz578UGZ
"""

import numpy as np
import librosa
import soundfile as sf
from scipy import signal
from scipy.fft import rfft, irfft
import torch
import torch.nn as nn
import os

class AudioPreprocessor:
    """
    Preprocessing pipeline for LSTM-based dereverberation.
    Implements Step 1 from the paper.
    """

    def __init__(self,
                 sample_rate=16000,
                 frame_length_ms=32,
                 frame_shift_ms=8,
                 n_fft=512,
                 normalize=True):
        """
        Initialize the audio preprocessor.

        Args:
            sample_rate: Audio sampling rate (Hz)
            frame_length_ms: Frame length in milliseconds (32ms as per paper)
            frame_shift_ms: Frame shift/hop in milliseconds (8ms as per paper)
            n_fft: FFT size (512 points as per paper)
            normalize: Whether to normalize features
        """
        self.sample_rate = sample_rate
        self.frame_length_ms = frame_length_ms
        self.frame_shift_ms = frame_shift_ms
        self.n_fft = n_fft
        self.normalize = normalize

        # Convert ms to samples
        self.frame_length = int(frame_length_ms * sample_rate / 1000)
        self.hop_length = int(frame_shift_ms * sample_rate / 1000)

        # Number of frequency bins (257 for 512-point FFT)
        self.n_freq_bins = n_fft // 2 + 1

        # Statistics for normalization (to be computed from training data)
        self.feature_mean = None
        self.feature_std = None

    def load_audio(self, audio_path):
        """Load audio file and resample to target sample rate."""
        audio, sr = librosa.load(audio_path, sr=self.sample_rate, mono=True)
        return audio

    def extract_magnitude_spectrum(self, audio):
        """
        Extract magnitude spectrum using STFT with Hamming window.

        Args:
            audio: Input audio signal (1D numpy array)

        Returns:
            magnitude: Magnitude spectrum (n_frames, n_freq_bins)
            phase: Phase spectrum (n_frames, n_freq_bins)
        """
        stft_matrix = librosa.stft(
            audio,
            n_fft=self.n_fft,
            hop_length=self.hop_length,
            win_length=self.frame_length,
            window='hamming',
            center=True,
            pad_mode='reflect'
        )

        magnitude = np.abs(stft_matrix).T
        phase = np.angle(stft_matrix).T

        return magnitude, phase

    def apply_cubic_root_compression(self, magnitude):
        """Apply cubic root compression to magnitude spectrum."""
        compressed = np.power(magnitude, 1.0/3.0)
        return compressed

    def normalize_features(self, features, compute_stats=False):
        """
        Normalize features to zero mean and unit variance.

        Args:
            features: Input features (n_frames, n_freq_bins)
            compute_stats: If True, compute and store mean/std from this data

        Returns:
            normalized: Normalized features
        """
        if compute_stats:
            self.feature_mean = np.mean(features, axis=0, keepdims=True)
            self.feature_std = np.std(features, axis=0, keepdims=True)
            self.feature_std = np.maximum(self.feature_std, 1e-8)

        if self.feature_mean is None or self.feature_std is None:
            raise ValueError("Normalization statistics not computed. Set compute_stats=True first.")

        normalized = (features - self.feature_mean) / self.feature_std
        return normalized

    def process_audio(self, audio, compute_stats=False, return_phase=True):
        """
        Complete preprocessing pipeline for a single audio signal.

        Args:
            audio: Input audio signal (1D numpy array) or path to audio file
            compute_stats: If True, compute normalization statistics
            return_phase: If True, return phase information

        Returns:
            features: Preprocessed features (n_frames, n_freq_bins)
            phase: Phase spectrum (if return_phase=True)
        """
        if isinstance(audio, str):
            audio = self.load_audio(audio)

        magnitude, phase = self.extract_magnitude_spectrum(audio)
        compressed = self.apply_cubic_root_compression(magnitude)

        if self.normalize:
            features = self.normalize_features(compressed, compute_stats=compute_stats)
        else:
            features = compressed

        if return_phase:
            return features, phase
        else:
            return features

    def compute_normalization_stats_from_dataset(self, audio_list):
        """
        Compute normalization statistics from a list of audio files/arrays.
        This should be called on ALL REVERBERANT training audio files.

        Args:
            audio_list: List of audio file paths or numpy arrays (reverberant only)
        """
        all_features = []

        for audio in audio_list:
            if isinstance(audio, str):
                audio = self.load_audio(audio)

            magnitude, _ = self.extract_magnitude_spectrum(audio)
            compressed = self.apply_cubic_root_compression(magnitude)
            all_features.append(compressed)

        all_features = np.concatenate(all_features, axis=0)
        self.feature_mean = np.mean(all_features, axis=0, keepdims=True)
        self.feature_std = np.std(all_features, axis=0, keepdims=True)
        self.feature_std = np.maximum(self.feature_std, 1e-8)


class TrainingTargetGenerator:
    """
    UNIFIED preprocessing for training pairs.
    Handles normalization correctly: normalize reverb (input), keep clean unnormalized (target).
    """
    def __init__(self, preprocessor):
        """
        Args:
            preprocessor: AudioPreprocessor instance with computed normalization stats
        """
        self.preprocessor = preprocessor
        self.sample_rate = preprocessor.sample_rate
        self.n_fft = preprocessor.n_fft
        self.frame_length = preprocessor.frame_length
        self.hop_length = preprocessor.hop_length
        self.n_freq_bins = preprocessor.n_freq_bins

    def generate_training_pair_from_real_data(self, clean_audio_path, reverb_audio_path):
        """
        Process clean + reverberant pair for training.
        IMPORTANT: Returns NORMALIZED reverb features and UNNORMALIZED clean features.

        Args:
            clean_audio_path: Path to clean/dry audio file
            reverb_audio_path: Path to reverberant audio file

        Returns:
            reverb_features: NORMALIZED cubic-root compressed reverberant features (input)
            target_features: UNNORMALIZED cubic-root compressed clean features (target)
            reverb_phase: Phase from reverberant audio (for reconstruction)
        """
        clean_audio = self.preprocessor.load_audio(clean_audio_path)
        reverb_audio = self.preprocessor.load_audio(reverb_audio_path)

        min_len = min(len(clean_audio), len(reverb_audio))
        clean_audio = clean_audio[:min_len]
        reverb_audio = reverb_audio[:min_len]

        clean_mag, _ = self.preprocessor.extract_magnitude_spectrum(clean_audio)
        reverb_mag, reverb_phase = self.preprocessor.extract_magnitude_spectrum(reverb_audio)

        clean_compressed = self.preprocessor.apply_cubic_root_compression(clean_mag)
        reverb_compressed = self.preprocessor.apply_cubic_root_compression(reverb_mag)

        if self.preprocessor.feature_mean is None:
            raise ValueError("Preprocessor must have normalization stats computed before processing pairs!")

        reverb_features = self.preprocessor.normalize_features(reverb_compressed, compute_stats=False)
        target_features = clean_compressed

        return reverb_features, target_features, reverb_phase


class RealDataPreparer:
    """
    Prepares real-world clean + reverberant pairs for training.
    Handles normalization statistics computation and pair processing.
    """
    def __init__(self, sample_rate=16000):
        self.sample_rate = sample_rate
        self.preprocessor = AudioPreprocessor(sample_rate=sample_rate, normalize=True)

    def prepare_dataset_from_folder(self, data_folder):
        """
        Expects folder structure:
        data_folder/
            clean/
                file1.wav, file2.wav, ...
            reverberant/
                file1.wav, file2.wav, ...

        Returns:
            reverb_features_list: List of normalized reverb features (inputs)
            target_features_list: List of unnormalized clean features (targets)
        """
        clean_dir = os.path.join(data_folder, 'clean')
        reverb_dir = os.path.join(data_folder, 'reverberant')

        clean_files = sorted([f for f in os.listdir(clean_dir) if f.endswith('.wav')])
        reverb_files = sorted([f for f in os.listdir(reverb_dir) if f.endswith('.wav')])

        reverb_paths = [os.path.join(reverb_dir, f) for f in reverb_files]
        self.preprocessor.compute_normalization_stats_from_dataset(reverb_paths)

        reverb_features_list = []
        target_features_list = []

        target_gen = TrainingTargetGenerator(preprocessor=self.preprocessor)

        for clean_file, reverb_file in zip(clean_files, reverb_files):
            clean_path = os.path.join(clean_dir, clean_file)
            reverb_path = os.path.join(reverb_dir, reverb_file)

            rev_feat, target_feat, _ = target_gen.generate_training_pair_from_real_data(
                clean_path, reverb_path
            )

            reverb_features_list.append(rev_feat)
            target_features_list.append(target_feat)

        return reverb_features_list, target_features_list


# === STEP 1 & 2 EXECUTION ===
if __name__ == "__main__":
    try:
        import google.colab
        IN_COLAB = True
    except:
        IN_COLAB = False

    if IN_COLAB:
        from google.colab import drive
        drive.mount('/content/drive')
        drive_base = '/content/drive/MyDrive/dereverberation_dataset'
        train_folder = os.path.join(drive_base, 'train')
        test_folder = os.path.join(drive_base, 'test')
    else:
        train_folder = './train'
        test_folder = './test'

    train_clean_dir = os.path.join(train_folder, 'clean')
    train_reverb_dir = os.path.join(train_folder, 'reverberant')
    test_reverb_dir = os.path.join(test_folder, 'reverberant')

    if not os.path.exists(train_clean_dir) or not os.path.exists(train_reverb_dir):
        print(f"ERROR: Required training directory structure not found!")
        exit(1)

    has_test_data = os.path.exists(test_reverb_dir)

    clean_files = sorted([f for f in os.listdir(train_clean_dir) if f.lower().endswith(('.wav', '.flac', '.mp3', '.m4a', '.ogg'))])
    reverb_files = sorted([f for f in os.listdir(train_reverb_dir) if f.lower().endswith(('.wav', '.flac', '.mp3', '.m4a', '.ogg'))])

    if not clean_files or not reverb_files:
        print(f"ERROR: No audio files found in train/clean/ or train/reverberant/")
        exit(1)

    print(f"Training pairs: {len(clean_files)} clean, {len(reverb_files)} reverberant")

    if has_test_data:
        test_reverb_files = sorted([f for f in os.listdir(test_reverb_dir) if f.lower().endswith(('.wav', '.flac', '.mp3', '.m4a', '.ogg'))])
        if test_reverb_files:
            print(f"Test files: {len(test_reverb_files)}")

    preprocessor = AudioPreprocessor(
        sample_rate=16000,
        frame_length_ms=32,
        frame_shift_ms=8,
        n_fft=512,
        normalize=True
    )

    reverb_paths = [os.path.join(train_reverb_dir, f) for f in reverb_files]
    preprocessor.compute_normalization_stats_from_dataset(reverb_paths)

    target_gen = TrainingTargetGenerator(preprocessor=preprocessor)
    reverb_features_list = []
    target_features_list = []
    phase_list = []

    try:
        for clean_file, reverb_file in zip(clean_files, reverb_files):
            clean_path = os.path.join(train_clean_dir, clean_file)
            reverb_path = os.path.join(train_reverb_dir, reverb_file)

            reverb_features, target_features, reverb_phase = target_gen.generate_training_pair_from_real_data(
                clean_path, reverb_path
            )

            reverb_features_list.append(reverb_features)
            target_features_list.append(target_features)
            phase_list.append(reverb_phase)

        print(f"Training dataset: {len(reverb_features_list)} pairs")
        print(f"Input shape: {reverb_features_list[0].shape}, Target shape: {target_features_list[0].shape}")

        if has_test_data:
            test_reverb_features_list = []
            test_phase_list = []
            test_filenames = []

            for reverb_file in test_reverb_files:
                reverb_path = os.path.join(test_reverb_dir, reverb_file)

                reverb_audio = preprocessor.load_audio(reverb_path)
                reverb_mag, reverb_phase = preprocessor.extract_magnitude_spectrum(reverb_audio)
                reverb_compressed = preprocessor.apply_cubic_root_compression(reverb_mag)
                reverb_features = preprocessor.normalize_features(reverb_compressed, compute_stats=False)

                test_reverb_features_list.append(reverb_features)
                test_phase_list.append(reverb_phase)
                test_filenames.append(reverb_file)

            print(f"Test dataset: {len(test_reverb_features_list)} files")

    except Exception as e:
        print(f"Error in processing: {e}")
        import traceback
        traceback.print_exc()


class LSTMDereverberation(nn.Module):
    """
    LSTM-based Speech Dereverberation Model.
    Implements Step 3 from the paper.
    """

    def __init__(self,
                 input_size=257,
                 hidden_size=512,
                 num_layers=2,
                 dropout=0.3,
                 weight_dropout=0.5):
        super(LSTMDereverberation, self).__init__()

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = dropout
        self.weight_dropout = weight_dropout

        self.lstm_layers = nn.ModuleList()

        for i in range(num_layers):
            layer_input_size = input_size if i == 0 else hidden_size

            lstm = nn.LSTM(
                input_size=layer_input_size,
                hidden_size=hidden_size,
                num_layers=1,
                batch_first=True,
                dropout=0
            )

            lstm = WeightDropLSTM(lstm, dropout=weight_dropout)
            self.lstm_layers.append(lstm)

        self.dropout_layer = nn.Dropout(dropout)
        self.linear = nn.Linear(hidden_size, input_size)
        self.relu = nn.ReLU()

        self._init_weights()

    def _init_weights(self):
        """Orthogonal initialization as per paper"""
        for name, param in self.named_parameters():
            if 'weight_hh_raw' in name:
                nn.init.orthogonal_(param)
            elif 'weight_ih' in name:
                nn.init.orthogonal_(param)
            elif 'weight' in name and param.dim() >= 2:
                nn.init.orthogonal_(param)
            elif 'bias' in name:
                nn.init.zeros_(param)

    def forward(self, x, hidden_states=None):
        """
        Forward pass through the LSTM dereverberation model.

        Args:
            x: Input features (batch_size, seq_len, input_size)
            hidden_states: Optional list of (h, c) tuples for each LSTM layer

        Returns:
            output: Enhanced features (batch_size, seq_len, input_size)
            new_hidden_states: Updated hidden states for each layer
        """
        batch_size, seq_len, _ = x.shape

        if hidden_states is None:
            hidden_states = self._init_hidden(batch_size, x.device)

        lstm_out = x
        new_hidden_states = []

        for i, lstm in enumerate(self.lstm_layers):
            lstm_out, (h, c) = lstm(lstm_out, hidden_states[i])
            new_hidden_states.append((h, c))

            if i < self.num_layers - 1:
                lstm_out = self.dropout_layer(lstm_out)

        projected = self.linear(lstm_out)
        output = self.relu(projected)

        return output, new_hidden_states

    def _init_hidden(self, batch_size, device):
        """Initialize hidden states for all LSTM layers"""
        hidden_states = []
        for _ in range(self.num_layers):
            h_0 = torch.zeros(1, batch_size, self.hidden_size, device=device)
            c_0 = torch.zeros(1, batch_size, self.hidden_size, device=device)
            hidden_states.append((h_0, c_0))
        return hidden_states

    def count_parameters(self):
        """Count total trainable parameters"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

    def predict(self, reverb_features, return_hidden=False):
        """
        Inference mode prediction.

        Args:
            reverb_features: Reverberant features (seq_len, input_size) or (batch_size, seq_len, input_size)
            return_hidden: Whether to return hidden states

        Returns:
            enhanced_features: Enhanced/denoised features
            hidden_states: (optional) Hidden states if return_hidden=True
        """
        if reverb_features.dim() == 2:
            reverb_features = reverb_features.unsqueeze(0)

        self.eval()
        with torch.no_grad():
            enhanced_features, hidden_states = self.forward(reverb_features)

        if return_hidden:
            return enhanced_features, hidden_states
        else:
            return enhanced_features


class WeightDropLSTM(nn.Module):
    """
    LSTM with weight dropout applied to recurrent connections.
    Implements variational dropout on the hidden-to-hidden weights.
    """
    def __init__(self, lstm, dropout=0.5):
        super(WeightDropLSTM, self).__init__()
        self.lstm = lstm
        self.dropout = dropout

        w_hh = self.lstm.weight_hh_l0.data.clone()
        self.weight_hh_raw = nn.Parameter(w_hh)

        del self.lstm._parameters['weight_hh_l0']

    def forward(self, x, hidden=None):
        """
        Forward pass with weight dropout applied.

        During training: Apply dropout mask to recurrent weights
        During evaluation: Use full weights without dropout
        """
        if self.training and self.dropout > 0:
            mask = self.weight_hh_raw.new_ones(self.weight_hh_raw.size()).bernoulli_(1 - self.dropout)
            w_hh = self.weight_hh_raw * mask / (1 - self.dropout)
        else:
            w_hh = self.weight_hh_raw

        self.lstm.weight_hh_l0 = w_hh
        output, hidden = self.lstm(x, hidden)

        return output, hidden


class DereverberationLoss(nn.Module):
    """
    Loss function for dereverberation training.
    Uses MSE loss in the cubic-root compressed magnitude domain.
    """
    def __init__(self, loss_type='mse'):
        super(DereverberationLoss, self).__init__()
        self.loss_type = loss_type
        if loss_type == 'mse':
            self.criterion = nn.MSELoss()
        else:
            raise ValueError(f"Unknown loss type: {loss_type}")

    def forward(self, predicted, target):
        """
        Compute loss between predicted and target features.

        Args:
            predicted: Model output (batch_size, seq_len, input_size)
            target: Target clean features (batch_size, seq_len, input_size)

        Returns:
            loss: Scalar loss value
        """
        return self.criterion(predicted, target)


from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from tqdm import tqdm

class DereverberationDataset(Dataset):
    """
    Dataset for speech dereverberation training with clean+reverb pairs.

    Each sample contains:
    - reverb_features: Normalized, cubic-root compressed magnitude spectrum (reverberant)
    - target_features: Cubic-root compressed magnitude spectrum (clean, NOT normalized)
    """

    def __init__(self, reverb_features_list, target_features_list):
        """
        Args:
            reverb_features_list: List of numpy arrays, each shape (seq_len, 257)
            target_features_list: List of numpy arrays, each shape (seq_len, 257)
        """
        assert len(reverb_features_list) == len(target_features_list), \
            f"Mismatch: {len(reverb_features_list)} reverb vs {len(target_features_list)} target files"

        self.reverb_features = []
        self.target_features = []

        for rev_feat, tgt_feat in zip(reverb_features_list, target_features_list):
            if isinstance(rev_feat, np.ndarray):
                rev_feat = torch.from_numpy(rev_feat).float()
            if isinstance(tgt_feat, np.ndarray):
                tgt_feat = torch.from_numpy(tgt_feat).float()

            self.reverb_features.append(rev_feat)
            self.target_features.append(tgt_feat)

        self.num_samples = len(self.reverb_features)

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        """
        Returns:
            reverb: (seq_len, 257) - normalized, cubic-root compressed (reverberant)
            target: (seq_len, 257) - cubic-root compressed (clean, NOT normalized)
            seq_len: scalar - length of sequence
        """
        reverb = self.reverb_features[idx]
        target = self.target_features[idx]
        seq_len = reverb.shape[0]

        return reverb, target, seq_len


def collate_fn_variable_length(batch):
    """
    Collate function for variable-length sequences.

    Args:
        batch: List of tuples (reverb, target, seq_len)

    Returns:
        reverb_padded: (batch_size, max_seq_len, 257)
        target_padded: (batch_size, max_seq_len, 257)
        lengths: (batch_size,) - actual lengths before padding
    """
    reverb_list = [item[0] for item in batch]
    target_list = [item[1] for item in batch]
    lengths = torch.tensor([item[2] for item in batch])

    reverb_padded = pad_sequence(reverb_list, batch_first=True, padding_value=0.0)
    target_padded = pad_sequence(target_list, batch_first=True, padding_value=0.0)

    return reverb_padded, target_padded, lengths


class DereverberationTrainer:
    """
    Trainer for LSTM-based dereverberation model.
    Implements Step 4 from the paper.
    """

    def __init__(self,
                 model,
                 train_dataset,
                 val_dataset=None,
                 batch_size=8,
                 learning_rate=0.001,
                 device='cuda' if torch.cuda.is_available() else 'cpu',
                 checkpoint_dir=None):
        """
        Args:
            model: LSTMDereverberation model (from Step 3)
            train_dataset: Training dataset (DereverberationDataset)
            val_dataset: Validation dataset (DereverberationDataset) - optional
            batch_size: Batch size (8 as per paper)
            learning_rate: Learning rate for Adam optimizer
            device: Device to train on
            checkpoint_dir: Directory to save checkpoints (Google Drive path)
        """
        self.model = model.to(device)
        self.device = device
        self.batch_size = batch_size

        if checkpoint_dir is None:
            try:
                from google.colab import drive
                drive.mount('/content/drive')
                self.checkpoint_dir = '/content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints'
            except:
                self.checkpoint_dir = './dereverberation_checkpoints'
        else:
            self.checkpoint_dir = checkpoint_dir

        self.train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            collate_fn=collate_fn_variable_length,
            num_workers=0,
            pin_memory=True if device == 'cuda' else False
        )

        self.has_val = val_dataset is not None and len(val_dataset) > 0
        if self.has_val:
            self.val_loader = DataLoader(
                val_dataset,
                batch_size=batch_size,
                shuffle=False,
                collate_fn=collate_fn_variable_length,
                num_workers=0,
                pin_memory=True if device == 'cuda' else False
            )
        else:
            self.val_loader = None

        self.criterion = nn.MSELoss(reduction='mean')

        self.optimizer = torch.optim.Adam(
            self.model.parameters(),
            lr=learning_rate
        )

        os.makedirs(self.checkpoint_dir, exist_ok=True)

        self.train_losses = []
        self.val_losses = []
        self.best_val_loss = float('inf')

        print(f"Trainer initialized: Device={device}, Batch size={batch_size}, LR={learning_rate}")
        print(f"Training samples: {len(train_dataset)}, Validation: {len(val_dataset) if self.has_val else 0}")

    def train_epoch(self, epoch):
        """
        Train for one epoch.

        Args:
            epoch: Current epoch number

        Returns:
            avg_loss: Average training loss for the epoch
        """
        self.model.train()
        total_loss = 0.0
        num_batches = 0

        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch} [Train]")

        for batch_idx, (reverb, target, lengths) in enumerate(pbar):
            reverb = reverb.to(self.device)
            target = target.to(self.device)
            lengths = lengths.to(self.device)

            self.optimizer.zero_grad()

            predicted, _ = self.model(reverb)

            loss = self.compute_loss_with_masking(predicted, target, lengths)

            loss.backward()

            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)

            self.optimizer.step()

            total_loss += loss.item()
            num_batches += 1

            pbar.set_postfix({'loss': f'{loss.item():.4f}'})

        avg_loss = total_loss / num_batches
        return avg_loss

    def validate(self, epoch):
        """
        Validate the model.

        Args:
            epoch: Current epoch number

        Returns:
            avg_loss: Average validation loss (or None if no validation set)
        """
        if not self.has_val:
            return None

        self.model.eval()
        total_loss = 0.0
        num_batches = 0

        pbar = tqdm(self.val_loader, desc=f"Epoch {epoch} [Val]")

        with torch.no_grad():
            for reverb, target, lengths in pbar:
                reverb = reverb.to(self.device)
                target = target.to(self.device)
                lengths = lengths.to(self.device)

                predicted, _ = self.model(reverb)

                loss = self.compute_loss_with_masking(predicted, target, lengths)

                total_loss += loss.item()
                num_batches += 1

                pbar.set_postfix({'loss': f'{loss.item():.4f}'})

        avg_loss = total_loss / num_batches
        return avg_loss

    def compute_loss_with_masking(self, predicted, target, lengths):
        """
        Compute MSE loss only on non-padded frames.

        Args:
            predicted: (batch_size, max_seq_len, 257)
            target: (batch_size, max_seq_len, 257)
            lengths: (batch_size,) - actual sequence lengths

        Returns:
            loss: Scalar loss value
        """
        batch_size, max_seq_len, feat_dim = predicted.shape

        mask = torch.arange(max_seq_len, device=self.device).unsqueeze(0) < lengths.unsqueeze(1)
        mask = mask.unsqueeze(-1).expand(-1, -1, feat_dim)

        predicted_masked = predicted * mask
        target_masked = target * mask

        squared_diff = (predicted_masked - target_masked) ** 2

        total_elements = mask.sum()
        loss = squared_diff.sum() / total_elements

        return loss

    def train(self, num_epochs, save_every=5):
        """
        Train the model for multiple epochs.

        Args:
            num_epochs: Number of epochs to train
            save_every: Save checkpoint every N epochs
        """
        for epoch in range(1, num_epochs + 1):
            train_loss = self.train_epoch(epoch)
            self.train_losses.append(train_loss)

            val_loss = self.validate(epoch) if self.has_val else None
            if val_loss is not None:
                self.val_losses.append(val_loss)

            print(f"Epoch {epoch}/{num_epochs} - Train Loss: {train_loss:.4f}" +
                  (f", Val Loss: {val_loss:.4f}" if val_loss is not None else ""))

            loss_to_compare = val_loss if val_loss is not None else train_loss
            if loss_to_compare < self.best_val_loss:
                self.best_val_loss = loss_to_compare
                self.save_checkpoint(epoch, is_best=True)

            if epoch % save_every == 0:
                self.save_checkpoint(epoch, is_best=False)

        print(f"Training complete. Best loss: {self.best_val_loss:.4f}")

    def save_checkpoint(self, epoch, is_best=False):
        """
        Save model checkpoint to Google Drive.

        Args:
            epoch: Current epoch
            is_best: Whether this is the best model so far
        """
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'best_val_loss': self.best_val_loss
        }

        if is_best:
            path = os.path.join(self.checkpoint_dir, 'best_model.pt')
        else:
            path = os.path.join(self.checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')

        torch.save(checkpoint, path)

    def load_checkpoint(self, checkpoint_path=None):
        """
        Load model checkpoint from Google Drive.

        Args:
            checkpoint_path: Path to checkpoint file (if None, loads best_model.pt)
        """
        if checkpoint_path is None:
            checkpoint_path = os.path.join(self.checkpoint_dir, 'best_model.pt')

        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.train_losses = checkpoint['train_losses']
        self.val_losses = checkpoint['val_losses']
        self.best_val_loss = checkpoint['best_val_loss']

        print(f"Loaded checkpoint from epoch {checkpoint['epoch']}, best loss: {self.best_val_loss:.4f}")


# === STEP 4 EXECUTION ===
if __name__ == "__main__":
    if 'reverb_features_list' in locals() and 'target_features_list' in locals():
        num_samples = len(reverb_features_list)

        if num_samples == 1:
            train_dataset = DereverberationDataset(reverb_features_list, target_features_list)
            val_dataset = None
            batch_size = 1
            num_epochs = 10
        elif num_samples < 10:
            train_dataset = DereverberationDataset(reverb_features_list, target_features_list)
            val_dataset = None
            batch_size = min(8, num_samples)
            num_epochs = 10
        else:
            split_idx = int(0.8 * num_samples)
            train_dataset = DereverberationDataset(
                reverb_features_list[:split_idx],
                target_features_list[:split_idx]
            )
            val_dataset = DereverberationDataset(
                reverb_features_list[split_idx:],
                target_features_list[split_idx:]
            )
            batch_size = 8
            num_epochs = 10

        model = LSTMDereverberation(
            input_size=257,
            hidden_size=512,
            num_layers=2,
            dropout=0.3,
            weight_dropout=0.5
        )

        try:
            from google.colab import drive
            drive.mount('/content/drive')
            checkpoint_dir = '/content/drive/MyDrive/Serveroperation_dataset/dereverberation_checkpoints'
        except:
            checkpoint_dir = './dereverberation_checkpoints'

        trainer = DereverberationTrainer(
            model=model,
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            batch_size=batch_size,
            learning_rate=0.001,
            checkpoint_dir=checkpoint_dir
        )

        trainer.train(num_epochs=num_epochs, save_every=10)

        # Post-training evaluation
        best_model_path = os.path.join(checkpoint_dir, 'best_model.pt')
        trainer.load_checkpoint(best_model_path)

        trainer.model.eval()

        eval_dataset = val_dataset if val_dataset else train_dataset
        eval_loader = DataLoader(
            eval_dataset,
            batch_size=1,
            shuffle=False,
            collate_fn=collate_fn_variable_length
        )

        total_mse = 0.0
        total_mae = 0.0
        num_eval_samples = 0

        with torch.no_grad():
            for reverb, target, lengths in eval_loader:
                reverb = reverb.to(trainer.device)
                target = target.to(trainer.device)
                lengths = lengths.to(trainer.device)

                predicted, _ = trainer.model(reverb)

                batch_size, max_seq_len, feat_dim = predicted.shape
                mask = torch.arange(max_seq_len, device=trainer.device).unsqueeze(0) < lengths.unsqueeze(1)
                mask = mask.unsqueeze(-1).expand(-1, -1, feat_dim)

                mse = ((predicted - target) ** 2 * mask).sum() / mask.sum()
                total_mse += mse.item()

                mae = (torch.abs(predicted - target) * mask).sum() / mask.sum()
                total_mae += mae.item()

                num_eval_samples += 1

        avg_mse = total_mse / num_eval_samples
        avg_mae = total_mae / num_eval_samples
        rmse = np.sqrt(avg_mse)
        snr_improvement_estimate = -10 * np.log10(avg_mse + 1e-10)

        print(f"\nEvaluation Metrics ({num_eval_samples} samples):")
        print(f"  MSE: {avg_mse:.6f}, RMSE: {rmse:.6f}, MAE: {avg_mae:.6f}")
        print(f"  SNR Improvement (est): {snr_improvement_estimate:.2f} dB")


class DereverberationInference:
    """
    Inference pipeline for LSTM-based speech dereverberation.
    Implements Steps 5 & 6 from the paper.
    """

    def __init__(self,
                 model_checkpoint_path,
                 preprocessor,
                 device='cuda' if torch.cuda.is_available() else 'cpu'):
        """Initialize inference pipeline."""
        self.device = device
        self.preprocessor = preprocessor

        if self.preprocessor.feature_mean is None or self.preprocessor.feature_std is None:
            raise ValueError("Preprocessor must have normalization statistics computed.")

        self.model = self._load_model(model_checkpoint_path)
        self.model.eval()

    def _load_model(self, checkpoint_path):
        """Load trained model from checkpoint."""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        model = LSTMDereverberation(
            input_size=257,
            hidden_size=512,
            num_layers=2,
            dropout=0.3,
            weight_dropout=0.5
        )

        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(self.device)

        print(f"Model loaded (epoch {checkpoint['epoch']}, loss: {checkpoint['best_val_loss']:.4f})")

        return model

    def enhance_audio(self, audio_input):
        """
        Complete enhancement pipeline: Steps 5 & 6.
        """
        if isinstance(audio_input, str):
            audio, sr = librosa.load(audio_input, sr=self.preprocessor.sample_rate, mono=True)
        else:
            audio = audio_input

        magnitude_reverb, phase_reverb = self.preprocessor.extract_magnitude_spectrum(audio)
        compressed_reverb = self.preprocessor.apply_cubic_root_compression(magnitude_reverb)
        normalized_reverb = self.preprocessor.normalize_features(compressed_reverb, compute_stats=False)

        input_tensor = torch.from_numpy(normalized_reverb).float()
        input_tensor = input_tensor.unsqueeze(0).to(self.device)

        with torch.no_grad():
            enhanced_normalized, _ = self.model(input_tensor)

        enhanced_normalized = enhanced_normalized.squeeze(0).cpu().numpy()

        enhanced_compressed = self._denormalize(enhanced_normalized)
        enhanced_magnitude = np.power(enhanced_compressed, 3.0)
        enhanced_complex = enhanced_magnitude * np.exp(1j * phase_reverb)
        enhanced_audio = self._inverse_stft(enhanced_complex.T)

        max_val = np.max(np.abs(enhanced_audio))
        if max_val > 0:
            enhanced_audio = enhanced_audio / max_val * 0.95

        return enhanced_audio

    def _denormalize(self, normalized_features):
        """Denormalize features using training statistics."""
        mean = self.preprocessor.feature_mean
        std = self.preprocessor.feature_std
        denormalized = normalized_features * std + mean
        return denormalized

    def _inverse_stft(self, complex_spectrum):
        """Reconstruct time-domain signal from complex spectrum."""
        audio = librosa.istft(
            complex_spectrum,
            hop_length=self.preprocessor.hop_length,
            win_length=self.preprocessor.frame_length,
            window='hamming',
            center=True
        )
        return audio

    def save_audio(self, audio, output_path, sample_rate=None):
        """Save audio to WAV file."""
        if sample_rate is None:
            sample_rate = self.preprocessor.sample_rate

        sf.write(output_path, audio, sample_rate)


# === STEP 5 & 6 EXECUTION ===
try:
    from google.colab import drive
    drive.mount('/content/drive')

    test_reverb_path = '/content/drive/MyDrive/dereverberation_dataset/test/reverberant'
    output_folder = '/content/drive/MyDrive/dereverberation_dataset/test/output'
    checkpoint_dir = '/content/drive/MyDrive/dereverberation_dataset/dereverberation_checkpoints'

    os.makedirs(output_folder, exist_ok=True)

    test_audio_files = []
    for f in os.listdir(test_reverb_path):
        if f.lower().endswith(('.wav', '.flac', '.mp3', '.m4a', '.ogg')):
            test_audio_files.append(f)

    if not test_audio_files:
        print("No audio files found in test/reverberant folder")
    else:
        checkpoint_files = []
        for f in os.listdir(checkpoint_dir):
            if f.endswith('.pt'):
                checkpoint_files.append(os.path.join(checkpoint_dir, f))

        if not checkpoint_files:
            print("No trained model found")
        else:
            model_checkpoint = [f for f in checkpoint_files if 'best_model' in f][0]

            preprocessor = AudioPreprocessor(
                sample_rate=16000,
                frame_length_ms=32,
                frame_shift_ms=8,
                n_fft=512,
                normalize=True
            )

            inference = DereverberationInference(
                model_checkpoint_path=model_checkpoint,
                preprocessor=preprocessor
            )

            for audio_file in test_audio_files:
                input_audio_path = os.path.join(test_reverb_path, audio_file)
                output_filename = f"enhanced_{os.path.splitext(audio_file)[0]}.wav"
                output_path = os.path.join(output_folder, output_filename)

                enhanced_audio = inference.enhance_audio(input_audio_path)
                inference.save_audio(enhanced_audio, output_path)

            print(f"Processed {len(test_audio_files)} files. Output: {output_folder}")

except Exception as e:
    print(f"Error: {e}")