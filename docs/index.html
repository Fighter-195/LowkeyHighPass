<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Signal Processing Cup 2026 | IIT Kanpur Lowkey High-Pass | Audio Demo with Metrics</title>
  <style>
    body { font-family: "Inter", sans-serif; background: #0d1117; color: #c9d1d9; margin: 0; padding: 40px; }
    h1, h2, h3 { text-align: center; margin: 0; }
    h1 { color: #58a6ff; font-size: 2.2em; margin-bottom: 0.2em; }
    h2 { color: #79c0ff; font-size: 1.6em; margin-bottom: 0.2em; }
    h3 { color: #c9d1d9; font-size: 1.1em; margin-bottom: 1em; }
    .container { max-width: 1200px; margin: 0 auto; }
    .controls { text-align: center; margin-bottom: 30px; }
    select { margin: 10px; padding: 10px; border-radius: 8px; border: none; background: #161b22; color: #c9d1d9; min-width: 200px; cursor: pointer; }
    button { margin: 10px; padding: 10px 20px; border-radius: 8px; border: none; background: #238636; color: white; cursor: pointer; font-weight: 600; }
    button:hover { background: #2ea043; }
    .method-description { text-align: center; margin: 20px auto 0; max-width: 800px; padding: 15px 20px; background: #161b22; border-radius: 8px; border-left: 3px solid #58a6ff; }
    .method-description p { margin: 0; color: #8b949e; font-size: 0.95em; line-height: 1.5; }
    .content-wrapper { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 30px; }
    .audio-box, .metrics-box { padding: 20px; background: #161b22; border-radius: 12px; box-shadow: 0 0 10px rgba(88, 166, 255, 0.12); }
    .audio-box h2, .metrics-box h2 { color: #58a6ff; margin-top: 0; }
    audio { width: 100%; margin: 10px 0; }
    table { width: 100%; border-collapse: collapse; margin-top: 12px; }
    th, td { padding: 10px; text-align: left; border-bottom: 1px solid #30363d; }
    th { color: #58a6ff; } td { color: #c9d1d9; }
    .metric-value { font-weight: 600; color: #7ee787; }
    #contentSection { display: none; }
    @media (max-width: 768px) { .content-wrapper { grid-template-columns: 1fr; } select { min-width: 160px; } }
  </style>
</head>
<body>
  <div class="container">
    <h1>Signal Processing Cup 2026</h1>
    <h2>IIT Kanpur : Lowkey High-Pass</h2>
    <h3>ðŸŽ§ Audio Demo with Metrics â€” choose method + mixed audio</h3>

    <div class="controls">
      <label for="method">Method:</label>
      <select id="method">
        <option value="">-- Choose Method --</option>
        <option value="neural_reverb">Neural Beamformer - Reverb</option>
        <option value="two_channel_audiozooming">Two Channel AudioZooming</option>
        <option value="neural_transformer_anechoic">Neural Transformer - Anechoic</option>
        <option value="sepformer_das_enh">Sepformer - Enh</option>
        <option value="sepformer_enh_enh">Sepformer - Enh+Enh</option>
        <option value="dsenet">DSENET</option>
        <option value="dfsnet">DFSNET</option>
        <option value="hybeam">HyBeam</option>
        <option value="student_teacher">Student Teacher</option>
      </select>

      <label for="mixedFile">Mixed audio:</label>
      <select id="mixedFile"><option value="">-- Choose Mixed File --</option></select>

      <button id="loadBtn">Load & Play</button>
    </div>

    <div class="method-description" id="methodDescription">
      <p id="descriptionText">Select a method to see its description here. This text provides information about the audio processing technique and its characteristics.</p>
    </div>

    <div id="contentSection">
      <div class="content-wrapper">
        <div>
          <div class="audio-box">
            <h2>Mixed Signal</h2>
            <audio id="mixedAudio" controls>
              <source id="mixedSource" type="audio/wav">
              Your browser does not support the audio element.
            </audio>
          </div>

          <div class="audio-box">
            <h2>Model Output</h2>
            <audio id="outputAudio" controls>
              <source id="outputSource" type="audio/wav">
              Your browser does not support the audio element.
            </audio>
          </div>
        </div>

        <div>
          <div class="metrics-box">
            <h2>Specific Metrics</h2>
            <table>
              <thead><tr><th>Metric</th><th>Score</th></tr></thead>
              <tbody>
                <tr><td>STOI</td><td class="metric-value" id="stoi">--</td></tr>
                <tr><td>PESQ</td><td class="metric-value" id="pesq">--</td></tr>
                <tr><td>SNR (dB)</td><td class="metric-value" id="snr">--</td></tr>
              </tbody>
            </table>
          </div>

          <div class="metrics-box">
            <h2>Time Complexity Metrics</h2>
            <table>
              <thead><tr><th>Metric</th><th>Value</th></tr></thead>
              <tbody>
                <tr><td>CPU RAM</td><td class="metric-value" id="cpuRam">--</td></tr>
                <tr><td>GPU VRAM</td><td class="metric-value" id="gpuVram">--</td></tr>
                <tr><td>MACs per sec</td><td class="metric-value" id="macsPerSec">--</td></tr>
                <tr><td>Complexity</td><td class="metric-value" id="complexity">--</td></tr>
              </tbody>
            </table>
          </div>
        </div>
      </div>
    </div>
  </div>

  <script>
    const baseURL = "https://raw.githubusercontent.com/ElectronicsClub-IITK/LowkeyHighPass/main/";
    const mixedFiles = ["male_female", "male_song", "male_noise1", "male_noise2"];

    const methodMetricsData = {
      neural_reverb: { stoi: 0.72, pesq: 1.05, snr: 3.7 },
      two_channel_audiozooming: { stoi: 0.61, pesq: 3.18, snr: 0.0 },
      neural_transformer_anechoic: { stoi: 0.853, pesq: 1.759, snr: 12.46 },
      sepformer_das_enh: { stoi: 0.814, pesq: 2.655, snr: -0.28 },
      sepformer_enh_enh: { stoi: 0.761, pesq: 2.077, snr: 6.79 },
      dsenet: { stoi: 0.80, pesq: 2.50, snr: 7.50 },
      dfsnet: { stoi: 0.80, pesq: 2.50, snr: 7.50 },
      hybeam: { stoi: 0.75, pesq: 1.29, snr: 4.56 },
      student_teacher: { stoi: 0.74, pesq: 1.45, snr: 5.69 }
    };

    const timeComplexityData = {
      neural_reverb: { cpuRam: '0.0 GB', gpuVram: '0.0 GB', macsPerSec: '0.0M', complexity: '0.0' },
      two_channel_audiozooming: { cpuRam: '2.87 MB', gpuVram: 'N/A', macsPerSec: '27.3 M', complexity: 'O (F*M^3)' },
      neural_transformer_anechoic: { cpuRam: '988.118 MB', gpuVram: '770.26 MB', macsPerSec: '216107.67M', complexity: 'O(L^2*d)' },
      sepformer_das_enh: { cpuRam: '1.7 GB', gpuVram: '0.0 GB', macsPerSec: '11368.87M', complexity: 'O (L^2*d)' },
      sepformer_enh_enh: { cpuRam: '1.7 GB', gpuVram: '0.0 GB', macsPerSec: '11368.87M', complexity: 'O (L^2*d)' },
      dsenet: { cpuRam: '1.5 GB', gpuVram: '0.0 GB', macsPerSec: '9500M', complexity: 'O (L^2*d)' },
      dfsnet: { cpuRam: '1.5 GB', gpuVram: '0.0 GB', macsPerSec: '9500M', complexity: 'O (L^2*d)' },
      hybeam: { cpuRam: '13.53 MB', gpuVram: '991.23 GB', macsPerSec: '11.2M', complexity: '0.0' },
      student_teacher: { cpuRam: '927.4 MB', gpuVram: '524 MB', macsPerSec: '87354 M', complexity: 'N A' }
    };

    const methodDescriptions = {
      neural_reverb: "Neural Beamformer with reverb handling uses deep learning to focus on target audio sources while managing room acoustics. This approach combines traditional beamforming with neural networks for improved speech extraction.",
      two_channel_audiozooming: "This uses tiny time/phase differences between mics to spatially focus on the target speaker, giving much better speech isolation and noise supression than any single-mic setup.",
      neural_transformer_anechoic: "Neural Transformer in anechoic conditions leverages self-attention mechanisms for audio separation. Trained on clean recordings, it excels at isolating individual sources with high fidelity.",
      sepformer_das_enh: "The sep_enh module first separates human speech from the mixture, then enhances the separated signal to clean leftover artifacts, giving higher PESQ/STOI than using separation or enhancement alone.
",
      sepformer_enh_enh: "Stacking the enhancement module twice cleans leftover artifacts and makes the human voice clearer and more crisp, increasing PESQ and STOI scores.",
      dsenet: "Deep Separation Enhancement Network (DSENET) uses deep learning architectures for robust audio source separation. This method achieves high-quality separation with moderate computational requirements.",
      dfsnet: "Deep Feature Separation Network (DFSNet) extracts and separates audio sources using hierarchical feature learning. This method achieves robust separation across various acoustic conditions with moderate computational requirements.",
      hybeam: "HyBeam cleans audio using a hybrid approach: delay-and-sum processing combined with an LSTM-based FT-JNF model.",
      student_teacher: "Studentâ€“teacher lets a smaller model mimic a big one: instead of learning from true labels, the student is trained to match the large teacher modelâ€™s outputs, making it suitable for edge devices."
    };

    const methodSelect = document.getElementById('method');
    const mixedSelect = document.getElementById('mixedFile');
    const loadBtn = document.getElementById('loadBtn');
    const contentSection = document.getElementById('contentSection');
    const mixedAudio = document.getElementById('mixedAudio');
    const mixedSource = document.getElementById('mixedSource');
    const outputAudio = document.getElementById('outputAudio');
    const outputSource = document.getElementById('outputSource');
    const stoiEl = document.getElementById('stoi');
    const pesqEl = document.getElementById('pesq');
    const snrEl = document.getElementById('snr');
    const cpuRamEl = document.getElementById('cpuRam');
    const gpuVramEl = document.getElementById('gpuVram');
    const macsEl = document.getElementById('macsPerSec');
    const complexityEl = document.getElementById('complexity');
    const descriptionText = document.getElementById('descriptionText');

    methodSelect.addEventListener('change', () => {
      mixedSelect.innerHTML = '<option value="">-- Choose Mixed File --</option>';
      const method = methodSelect.value;

      if (!method) {
        contentSection.style.display = 'none';
        return;
      }

      mixedFiles.forEach(m => {
        const opt = document.createElement('option');
        opt.value = m;
        opt.textContent = m.split('_').map(word => word.charAt(0).toUpperCase() + word.slice(1)).join('_');
        mixedSelect.appendChild(opt);
      });

      const m = methodMetricsData[method];
      stoiEl.textContent = m.stoi;
      pesqEl.textContent = m.pesq;
      snrEl.textContent = m.snr;

      const t = timeComplexityData[method];
      cpuRamEl.textContent = t.cpuRam;
      gpuVramEl.textContent = t.gpuVram;
      macsEl.textContent = t.macsPerSec;
      complexityEl.textContent = t.complexity;

      descriptionText.textContent = methodDescriptions[method] || "Select a method to see its description here.";
      contentSection.style.display = 'block';
    });

    function buildMixedUrl(mixedName) {
      return `${baseURL}mixed_audio/${mixedName}.wav`;
    }

    function buildOutputUrl(method, mixedName) {
      return `${baseURL}output_audio/${method}_${mixedName}.wav`;
    }

    loadBtn.addEventListener('click', () => {
      const method = methodSelect.value;
      const mixed = mixedSelect.value;
      if (!method || !mixed) return alert('Select both method and mixed file');

      mixedSource.src = buildMixedUrl(mixed);
      mixedAudio.load();
      outputSource.src = buildOutputUrl(method, mixed);
      outputAudio.load();

      contentSection.style.display = 'block';
    });

    [mixedAudio, outputAudio].forEach(a => {
      a.addEventListener('error', () => {
        console.error('Audio failed to load:', a.id, a.currentSrc, a.error ? a.error.code : 'unknown');
      });
      a.addEventListener('loadeddata', () => {
        console.log(a.id, 'loaded successfully:', a.currentSrc);
      });
    });
  </script>
</body>
</html>
